const fs = require('fs');
const path = require('path');
const { getMistralResponse } = require('../llm');

// Load the malware DB (dynamic path to avoid fs errors)
const malwareDBPath = path.join(__dirname, '..', 'malwareDB.json');

function getMalwareDB() {
  try {
    const data = fs.readFileSync(malwareDBPath, 'utf-8');
    return JSON.parse(data);
  } catch (err) {
    console.error('❌ Error loading malware DB:', err);
    return { malicious: [] };
  }
}

// Controller logic
async function checkMalware(req, res) {
  const packageName = req.body.packageName || req.body.name;

  if (!packageName) {
    return res.status(400).json({ error: 'Valid packageName is required' });
  }

  const db = getMalwareDB();
  const isMalicious = db.malicious.includes(packageName.toLowerCase());

  if (isMalicious) {
    return res.json({
      packageName,
      isMalicious: true,
      source: 'malwareDB'
    });
  }

  // Fallback to LLM analysis
  const mistralPrompt = `Assess the risk of the Android app named "${packageName}". Respond with a JSON object containing score (0–100), level (Low/Medium/High), and reasons.`;

  const llmResult = await getMistralResponse(mistralPrompt);

  let parsedResult;
  try {
    parsedResult = JSON.parse(llmResult);
  } catch (err) {
    console.error("⚠️ Failed to parse LLM response:", llmResult);
    return res.status(500).json({
      error: 'LLM returned an unstructured response',
      raw: llmResult
    });
  }

  return res.json({
    packageName,
    isMalicious: parsedResult.level === 'High' || parsedResult.score >= 80,
    source: 'mistral',
    llmResult: parsedResult
  });
}

module.exports = { checkMalware };